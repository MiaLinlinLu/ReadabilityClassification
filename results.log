Splitting texts...
Creating filtered texts... This might take up to one minute.
********************************************************************************
********************************************************************************
********************************************************************************
Preparing test data
Test data was prepared
Training models using Plain texts.
********************************************************************************
train A SVM 
*********************************************************
report of SVM model: 
              precision    recall  f1-score   support

    labels-0      0.000     0.000     0.000       682
    labels_1      0.234     0.054     0.088       722
    labels_2      0.444     0.006     0.011       722
    labels_3      0.111     0.001     0.003       713
    labels-4      0.198     0.971     0.329       681

    accuracy                          0.200      3520
   macro avg      0.197     0.206     0.086      3520
weighted avg      0.200     0.200     0.084      3520

********************************************************************************
Train B NB
*********************************************************
report of NB model: 
              precision    recall  f1-score   support

    labels-0      0.245     0.424     0.311       682
    labels_1      0.215     0.313     0.255       722
    labels_2      0.191     0.147     0.166       722
    labels_3      0.197     0.056     0.087       713
    labels-4      0.299     0.233     0.262       681

    accuracy                          0.233      3520
   macro avg      0.229     0.235     0.216      3520
weighted avg      0.228     0.233     0.215      3520

********************************************************************************
Train C RF
*********************************************************
report of Forest model: 
              precision    recall  f1-score   support

    labels-0      0.000     0.000     0.000       682
    labels_1      0.000     0.000     0.000       722
    labels_2      0.000     0.000     0.000       722
    labels_3      0.000     0.000     0.000       713
    labels-4      0.194     1.000     0.324       681

    accuracy                          0.193      3520
   macro avg      0.039     0.200     0.065      3520
weighted avg      0.037     0.193     0.063      3520

********************************************************************************
Train D LR
*********************************************************
report of LogisticRegression model: 
              precision    recall  f1-score   support

    labels-0      0.000     0.000     0.000       682
    labels_1      0.167     0.001     0.003       722
    labels_2      0.000     0.000     0.000       722
    labels_3      0.000     0.000     0.000       713
    labels-4      0.193     0.997     0.324       681

    accuracy                          0.193      3520
   macro avg      0.072     0.200     0.065      3520
weighted avg      0.072     0.193     0.063      3520

********************************************************************************
********************************************************************************
Training models using Filtered texts.
********************************************************************************
train A SVM 
*********************************************************
report of _fSVM model: 
              precision    recall  f1-score   support

    labels-0      0.236     0.145     0.180       702
    labels_1      0.189     0.238     0.211       672
    labels_2      0.232     0.119     0.157       717
    labels_3      0.216     0.135     0.166       721
    labels-4      0.220     0.444     0.294       708

    accuracy                          0.215      3520
   macro avg      0.219     0.216     0.202      3520
weighted avg      0.219     0.215     0.201      3520

********************************************************************************
Train B NB
*********************************************************
report of _fNB model: 
              precision    recall  f1-score   support

    labels-0      0.201     0.218     0.209       702
    labels_1      0.203     0.320     0.249       672
    labels_2      0.231     0.128     0.165       717
    labels_3      0.226     0.180     0.201       721
    labels-4      0.193     0.199     0.196       708

    accuracy                          0.208      3520
   macro avg      0.211     0.209     0.204      3520
weighted avg      0.211     0.208     0.203      3520

********************************************************************************
Train D LR
*********************************************************
report of _fLR model: 
              precision    recall  f1-score   support

    labels-0      0.243     0.120     0.160       702
    labels_1      0.185     0.188     0.186       672
    labels_2      0.270     0.046     0.079       717
    labels_3      0.225     0.057     0.091       721
    labels-4      0.218     0.674     0.329       708

    accuracy                          0.216      3520
   macro avg      0.228     0.217     0.169      3520
weighted avg      0.229     0.216     0.168      3520

********************************************************************************
Train C RF
*********************************************************
report of _fRF model: 
              precision    recall  f1-score   support

    labels-0      0.281     0.013     0.025       702
    labels_1      0.000     0.000     0.000       672
    labels_2      0.250     0.001     0.003       717
    labels_3      0.133     0.003     0.005       721
    labels-4      0.204     0.997     0.338       708

    accuracy                          0.204      3520
   macro avg      0.174     0.203     0.074      3520
weighted avg      0.175     0.204     0.075      3520

Scores
╒════╤════════════════════╤═════════════╤══════════╤══════════════╕
│    │ Name               │   Precision │   Recall │   F1 Measure │
╞════╪════════════════════╪═════════════╪══════════╪══════════════╡
│  0 │ SVM                │    19.7458  │  20.6318 │      8.61273 │
├────┼────────────────────┼─────────────┼──────────┼──────────────┤
│  1 │ NB                 │    22.9386  │  23.4634 │     21.6214  │
├────┼────────────────────┼─────────────┼──────────┼──────────────┤
│  2 │ Forest             │     3.87042 │  20      │      6.48571 │
├────┼────────────────────┼─────────────┼──────────┼──────────────┤
│  3 │ LogisticRegression │     7.19788 │  19.969  │      6.52932 │
╘════╧════════════════════╧═════════════╧══════════╧══════════════╛
F_Scores
╒════╤════════╤═════════════╤══════════╤══════════════╕
│    │ Name   │   Precision │   Recall │   F1 Measure │
╞════╪════════╪═════════════╪══════════╪══════════════╡
│  0 │ _fSVM  │     21.855  │  21.5996 │     20.1523  │
├────┼────────┼─────────────┼──────────┼──────────────┤
│  1 │ _fNB   │     21.0986 │  20.9132 │     20.3951  │
├────┼────────┼─────────────┼──────────┼──────────────┤
│  2 │ _fRF   │     17.362  │  20.2833 │      7.41547 │
├────┼────────┼─────────────┼──────────┼──────────────┤
│  3 │ _fLR   │     22.8415 │  21.6756 │     16.9075  │
╘════╧════════╧═════════════╧══════════╧══════════════╛
********************************************************************************
********************************************************************************
with ngram- Train models without removing stop words.
********************************************************************************
********************************************************************************
with ngram-Train models after removing stop words.
********************************************************************************
Prepare test data
Test data was prepared
********************************************************************************
train A SVM 
*********************************************************
report of ngramSVM model: 
              precision    recall  f1-score   support

    labels-0      0.296     0.283     0.290       682
    labels_1      0.237     0.252     0.244       722
    labels_2      0.199     0.193     0.196       722
    labels_3      0.232     0.248     0.240       713
    labels-4      0.329     0.310     0.319       681

    accuracy                          0.256      3520
   macro avg      0.259     0.257     0.258      3520
weighted avg      0.258     0.256     0.257      3520

********************************************************************************
Train B NB
*********************************************************
report of ngramNB model: 
              precision    recall  f1-score   support

    labels-0      0.311     0.327     0.319       682
    labels_1      0.248     0.271     0.259       722
    labels_2      0.224     0.220     0.222       722
    labels_3      0.251     0.304     0.275       713
    labels-4      0.367     0.236     0.287       681

    accuracy                          0.272      3520
   macro avg      0.280     0.272     0.273      3520
weighted avg      0.279     0.272     0.272      3520

********************************************************************************
Train C RF
*********************************************************
report of ngramForest model: 
              precision    recall  f1-score   support

    labels-0      0.287     0.324     0.305       682
    labels_1      0.260     0.107     0.151       722
    labels_2      0.227     0.122     0.159       722
    labels_3      0.230     0.196     0.212       713
    labels-4      0.281     0.602     0.384       681

    accuracy                          0.266      3520
   macro avg      0.257     0.270     0.242      3520
weighted avg      0.256     0.266     0.240      3520

********************************************************************************
Train D LR
*********************************************************
report of ngramLogisticRegression model: 
              precision    recall  f1-score   support

    labels-0      0.338     0.270     0.300       682
    labels_1      0.237     0.265     0.250       722
    labels_2      0.203     0.213     0.208       722
    labels_3      0.243     0.245     0.244       713
    labels-4      0.346     0.352     0.349       681

    accuracy                          0.268      3520
   macro avg      0.274     0.269     0.270      3520
weighted avg      0.272     0.268     0.269      3520

********************************************************************************
train A SVM 
*********************************************************
report of ngram_fSVM model: 
              precision    recall  f1-score   support

    labels-0      0.207     0.135     0.164       702
    labels_1      0.209     0.126     0.158       672
    labels_2      0.242     0.091     0.132       717
    labels_3      0.215     0.098     0.135       721
    labels-4      0.232     0.674     0.345       708

    accuracy                          0.225      3520
   macro avg      0.221     0.225     0.187      3520
weighted avg      0.221     0.225     0.187      3520

********************************************************************************
Train B NB
*********************************************************
report of ngram_fNB model: 
              precision    recall  f1-score   support

    labels-0      0.207     0.123     0.154       702
    labels_1      0.180     0.632     0.280       672
    labels_2      0.208     0.053     0.084       717
    labels_3      0.199     0.054     0.085       721
    labels-4      0.174     0.090     0.119       708

    accuracy                          0.185      3520
   macro avg      0.194     0.190     0.145      3520
weighted avg      0.194     0.185     0.143      3520

********************************************************************************
Train C RF
*********************************************************
report of ngram_fForest model: 
              precision    recall  f1-score   support

    labels-0      0.194     0.067     0.100       702
    labels_1      0.191     0.091     0.123       672
    labels_2      0.228     0.121     0.158       717
    labels_3      0.169     0.087     0.115       721
    labels-4      0.229     0.713     0.347       708

    accuracy                          0.217      3520
   macro avg      0.202     0.216     0.169      3520
weighted avg      0.202     0.217     0.169      3520

********************************************************************************
Train D LR
*********************************************************
report of ngram_fLogisticRegression model: 
              precision    recall  f1-score   support

    labels-0      0.199     0.123     0.152       702
    labels_1      0.211     0.210     0.211       672
    labels_2      0.223     0.091     0.129       717
    labels_3      0.221     0.046     0.076       721
    labels-4      0.234     0.655     0.345       708

    accuracy                          0.224      3520
   macro avg      0.218     0.225     0.182      3520
weighted avg      0.218     0.224     0.182      3520

Scores
╒════╤═════════════════════════╤═════════════╤══════════╤══════════════╕
│    │ Name                    │   Precision │   Recall │   F1 Measure │
╞════╪═════════════════════════╪═════════════╪══════════╪══════════════╡
│  0 │ ngramSVM                │     25.881  │  25.7135 │      25.7771 │
├────┼─────────────────────────┼─────────────┼──────────┼──────────────┤
│  1 │ ngramNB                 │     28.0216 │  27.1887 │      27.2581 │
├────┼─────────────────────────┼─────────────┼──────────┼──────────────┤
│  2 │ ngramForest             │     25.7047 │  27.0198 │      24.1926 │
├────┼─────────────────────────┼─────────────┼──────────┼──────────────┤
│  3 │ ngramLogisticRegression │     27.3563 │  26.91   │      27.0382 │
╘════╧═════════════════════════╧═════════════╧══════════╧══════════════╛
F_Scores
╒════╤═══════════════════════════╤═════════════╤══════════╤══════════════╕
│    │ Name                      │   Precision │   Recall │   F1 Measure │
╞════╪═══════════════════════════╪═════════════╪══════════╪══════════════╡
│  0 │ ngram_fSVM                │     22.0917 │  22.4935 │      18.6691 │
├────┼───────────────────────────┼─────────────┼──────────┼──────────────┤
│  1 │ ngram_fNB                 │     19.3681 │  19.0487 │      14.4599 │
├────┼───────────────────────────┼─────────────┼──────────┼──────────────┤
│  2 │ ngram_fForest             │     20.2342 │  21.5944 │      16.861  │
├────┼───────────────────────────┼─────────────┼──────────┼──────────────┤
│  3 │ ngram_fLogisticRegression │     21.771  │  22.4824 │      18.2443 │
╘════╧═══════════════════════════╧═════════════╧══════════╧══════════════╛
********************************************************************************
Fasttext Model
****************************************************************************************************
with TESTSIZE=0.4-------
train fasttext models with 1 gram
              precision    recall  f1-score   support

    labels-0      0.976     0.983     0.979       702
    labels_1      0.806     0.865     0.834       696
    labels_2      0.670     0.661     0.666       706
    labels_3      0.754     0.699     0.726       711
    labels-4      0.913     0.919     0.916       705

    accuracy                          0.825      3520
   macro avg      0.824     0.825     0.824      3520
weighted avg      0.823     0.825     0.824      3520

train fasttext models with 2 gram
              precision    recall  f1-score   support

    labels-0      0.982     0.989     0.985       702
    labels_1      0.847     0.872     0.859       696
    labels_2      0.721     0.739     0.730       706
    labels_3      0.788     0.744     0.766       711
    labels-4      0.923     0.918     0.920       705

    accuracy                          0.852      3520
   macro avg      0.852     0.852     0.852      3520
weighted avg      0.852     0.852     0.852      3520

train fasttext models with 3 gram
              precision    recall  f1-score   support

    labels-0      0.982     0.987     0.984       702
    labels_1      0.834     0.907     0.869       696
    labels_2      0.776     0.727     0.751       706
    labels_3      0.804     0.795     0.799       711
    labels-4      0.929     0.913     0.921       705

    accuracy                          0.865      3520
   macro avg      0.865     0.866     0.865      3520
weighted avg      0.865     0.865     0.865      3520

train fasttext models with 4 gram
              precision    recall  f1-score   support

    labels-0      0.976     0.984     0.980       702
    labels_1      0.727     0.944     0.821       696
    labels_2      0.795     0.548     0.649       706
    labels_3      0.799     0.814     0.806       711
    labels-4      0.927     0.915     0.921       705

    accuracy                          0.841      3520
   macro avg      0.845     0.841     0.835      3520
weighted avg      0.845     0.841     0.835      3520

train fasttext models with 5 gram
              precision    recall  f1-score   support

    labels-0      0.961     0.980     0.970       702
    labels_1      0.771     0.915     0.837       696
    labels_2      0.823     0.644     0.723       706
    labels_3      0.807     0.821     0.814       711
    labels-4      0.919     0.913     0.916       705

    accuracy                          0.855      3520
   macro avg      0.856     0.855     0.852      3520
weighted avg      0.856     0.855     0.852      3520

train fasttext models with 6 gram
              precision    recall  f1-score   support

    labels-0      0.948     0.984     0.966       702
    labels_1      0.766     0.904     0.829       696
    labels_2      0.841     0.646     0.731       706
    labels_3      0.812     0.823     0.818       711
    labels-4      0.908     0.912     0.910       705

    accuracy                          0.853      3520
   macro avg      0.855     0.854     0.851      3520
weighted avg      0.855     0.853     0.851      3520

train fasttext models with 7 gram
              precision    recall  f1-score   support

    labels-0      0.940     0.980     0.960       702
    labels_1      0.726     0.908     0.807       696
    labels_2      0.861     0.528     0.655       706
    labels_3      0.773     0.828     0.800       711
    labels-4      0.892     0.913     0.903       705

    accuracy                          0.831      3520
   macro avg      0.838     0.832     0.825      3520
weighted avg      0.838     0.831     0.825      3520

train fasttext models with 8 gram
              precision    recall  f1-score   support

    labels-0      0.934     0.980     0.956       702
    labels_1      0.755     0.894     0.818       696
    labels_2      0.839     0.615     0.710       706
    labels_3      0.792     0.788     0.790       711
    labels-4      0.878     0.915     0.896       705

    accuracy                          0.838      3520
   macro avg      0.839     0.838     0.834      3520
weighted avg      0.840     0.838     0.834      3520

********************************************************************************
---------------SVM With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of SVM with Cohesive Indices model: 
              precision    recall  f1-score   support

    labels-0      0.616     0.863     0.719       721
    labels_1      0.509     0.627     0.562       740
    labels_2      0.413     0.126     0.193       714
    labels_3      0.393     0.199     0.264       669
    labels-4      0.565     0.873     0.686       676

    accuracy                          0.539      3520
   macro avg      0.499     0.537     0.485      3520
weighted avg      0.500     0.539     0.487      3520

********************************************************************************
---------------TF-IDF With Cohesive Indices---------------------------
********************************************************************************
train A SVM 
*********************************************************
report of SVM with Cohesive Indices model: 
              precision    recall  f1-score   support

    labels-0      0.223     0.354     0.273       721
    labels_1      0.208     0.281     0.239       740
    labels_2      0.170     0.130     0.148       714
    labels_3      0.189     0.103     0.133       669
    labels-4      0.198     0.136     0.161       676

    accuracy                          0.204      3520
   macro avg      0.198     0.201     0.191      3520
weighted avg      0.198     0.204     0.193      3520

********************************************************************************
---------------Testing the best models on Weebit---------------------------
********************************************************************************
---------------Starting with BERT---------------------------
********************************************************************************
---------------With Fasttext---------------------------
              precision    recall  f1-score   support

    labels-0      0.426     0.526     0.471       700
    labels_1      0.156     0.091     0.115       646
    labels_2      0.422     0.120     0.187       807
    labels_3      0.309     0.758     0.439       789
    labels-4      0.691     0.178     0.283       629

    accuracy                          0.346      3571
   macro avg      0.401     0.335     0.299      3571
weighted avg      0.397     0.346     0.302      3571

